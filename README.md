# VLLMs Docker API

Una plataforma completa y lista para producciÃ³n que permite ejecutar mÃºltiples modelos de IA utilizando vLLM (Very Large Language Models) con arquitectura de microservicios. Este proyecto permite conectar mÃºltiples servidores vLLM para procesar diferentes tipos de modelos de IA de manera eficiente y escalable.

> **âš ï¸ IMPORTANTE: Esta soluciÃ³n requiere GPU NVIDIA obligatoriamente. No es compatible con sistemas que solo tienen CPU.**

## ğŸŒŸ CaracterÃ­sticas Principales

- ğŸš€ **Multi-Modelo Flexible**: Soporte para modelos de lenguaje, visiÃ³n, OCR, generaciÃ³n de texto, y mÃ¡s
- ğŸ“ **Procesamiento VersÃ¡til**: Maneja texto, imÃ¡genes, documentos PDF, y otros formatos
- ğŸ”„ **API RESTful**: Endpoints HTTP simples para integraciÃ³n fÃ¡cil
- ğŸ³ **Contenedorizado**: Despliegue sencillo con Docker Compose
- âš¡ **AceleraciÃ³n GPU**: Soporte NVIDIA CUDA para rendimiento Ã³ptimo (requerido)
- ğŸ“Š **Monitoreo de Salud**: Chequeos de salud y endpoints de estado
- ğŸ”§ **Configurable**: Ajustes flexibles de memoria GPU y parÃ¡metros de modelo
- ğŸ¯ **Alto Rendimiento**: Optimizado para GPUs NVIDIA con mÃ¡xima eficiencia

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   VLLMs Docker API                          â”‚
â”‚                   (Puerto 8000)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTP/REST
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Servidores vLLM                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     OCR         â”‚ â”‚   Lenguaje      â”‚ â”‚     VisiÃ³n      â”‚ â”‚
â”‚  â”‚   (OlmOCR)      â”‚ â”‚   (Llama)       â”‚ â”‚     (LLaVA)     â”‚ â”‚
â”‚  â”‚   (Puerto 8001) â”‚ â”‚   (Puerto 8002) â”‚ â”‚   (Puerto N)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤– Tipos de Modelos Soportados

Esta plataforma puede ejecutar diversos tipos de modelos de IA:

### ğŸ“ Modelos de Lenguaje (LLM)
- **GeneraciÃ³n de texto**: Llama, GPT, Falcon, Mistral
- **TraducciÃ³n automÃ¡tica**: NLLB, M2M100
- **Resumen de texto**: PEGASUS, BART
- **AnÃ¡lisis de sentimientos**: BERT, RoBERTa

### ğŸ‘ï¸ Modelos de VisiÃ³n-Lenguaje
- **OCR y documentos**: OlmOCR, TrOCR, Donut
- **DescripciÃ³n de imÃ¡genes**: LLaVA, BLIP, OpenFlamingo
- **AnÃ¡lisis visual**: GPT-4V, Claude-3
- **DetecciÃ³n de objetos**: OWL-ViT, GLIP

### ğŸ¯ Modelos Especializados
- **CÃ³digo**: CodeLlama, StarCoder, DeepSeek Coder
- **MatemÃ¡ticas**: MATH models, Minerva
- **Ciencia**: Galactica, scientific models
- **Audio**: Wav2CLIP, AudioLDM

## ğŸ“‹ Prerrequisitos

### Requisitos Obligatorios
- **Docker** y **Docker Compose**
- **Git** (para clonar el repositorio)
- **GPU NVIDIA** con soporte CUDA (requerido)
- **Drivers NVIDIA** instalados
- **Docker con soporte GPU** habilitado
- **Python 3.8+** (para desarrollo local)

### Especificaciones GPU Recomendadas
- **MÃ­nimo:** GPU con 8GB VRAM (RTX 3070, RTX 4060 Ti o superior)
- **Recomendado:** GPU con 12GB+ VRAM para modelos mÃ¡s grandes
- **Ã“ptimo:** GPU con 24GB+ VRAM para mÃ¡xima capacidad de procesamiento

## ğŸš€ Inicio RÃ¡pido

### 1. Clonar el Repositorio
```bash
git clone <repository-url>
cd vllms-docker-api
```

### 2. ConfiguraciÃ³n GPU

#### Configurar Modelos Disponibles
```bash
# Configurar modelo OlmOCR
cd vllm-models/olmocr
nano .env  # Ajustar parÃ¡metros segÃºn tu GPU

# Iniciar modelo OlmOCR
docker-compose up -d

# Volver al directorio raÃ­z
cd ../..

# Configurar la API para conectar con el modelo
nano vllm-api/.env
```

### 3. Verificar InstalaciÃ³n
```bash
# Verificar estado de la API
curl http://localhost:8000/health

# Listar modelos disponibles
curl http://localhost:8000/models

# InformaciÃ³n del servicio
curl http://localhost:8000/
```

### 4. Ejemplos de Uso

#### Procesar Documento con OCR
```bash
# Procesar documento PDF con modelo OCR
curl -X POST "http://localhost:8000/process?model=olmocr" \
  -F "file=@documento.pdf" \
  -F "output_format=markdown"
```

#### Generar Texto con LLM
```bash
# Generar texto con modelo de lenguaje
curl -X POST "http://localhost:8000/process?model=llama" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explica cuÃ¡ntica de manera simple", "max_tokens": 100}'
```

#### Describir Imagen con Modelo de VisiÃ³n
```bash
# Describir imagen con modelo visiÃ³n-lenguaje
curl -X POST "http://localhost:8000/process?model=llava" \
  -F "file=@imagen.jpg" \
  -F "prompt=Describe esta imagen en detalle"
```

## ğŸ“ Estructura del Proyecto

```
vllms-docker-api/
â”œâ”€â”€ README.md                    # Este archivo
â”œâ”€â”€ vllm-api/                   # API principal multi-modelo
â”‚   â”œâ”€â”€ app/                    # AplicaciÃ³n FastAPI
â”‚   â”‚   â”œâ”€â”€ main.py            # Endpoints y lÃ³gica de la API
â”‚   â”‚   â”œâ”€â”€ requirements.txt   # Dependencias Python
â”‚   â”‚   â””â”€â”€ models/            # Modelos y configuraciÃ³n
â”‚   â”œâ”€â”€ Dockerfile             # DefiniciÃ³n del contenedor API
â”‚   â”œâ”€â”€ docker-compose.yml     # ConfiguraciÃ³n GPU
â”‚   â”œâ”€â”€ .env                   # Variables de entorno API
â”‚   â””â”€â”€ README.md              # DocumentaciÃ³n detallada de la API
â”œâ”€â”€ vllm-models/               # Configuraciones de modelos vLLM
â”‚   â”œâ”€â”€ README.md              # GuÃ­a de modelos
â”‚   â”œâ”€â”€ model_cache/           # Cache de modelos (volumen Docker)
â”‚   â”œâ”€â”€ olmocr/               # Modelo OCR (allenai/olmOCR-7B-0825-FP8)
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml # ConfiguraciÃ³n vLLM especÃ­fica
â”‚   â”‚   â”œâ”€â”€ .env.example       # Variables de entorno del modelo
â”‚   â”‚   â””â”€â”€ README.md          # DocumentaciÃ³n del modelo
â”‚   â””â”€â”€ llama-7b/             # Modelo de lenguaje (meta-llama/Llama-2-7b)
â”‚       â”œâ”€â”€ docker-compose.yml # ConfiguraciÃ³n vLLM especÃ­fica
â”‚       â”œâ”€â”€ .env.example       # Variables de entorno del modelo
â”‚       â””â”€â”€ README.md          # DocumentaciÃ³n del modelo
â””â”€â”€ workspace/                 # Espacio de trabajo para procesamiento
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno Principales

#### ConfiguraciÃ³n de la API (`vllm-api/.env`)
```bash
# ConfiguraciÃ³n general
DEFAULT_GPU_UTIL=0.90
DEFAULT_MAX_LEN=18608

# Servidores vLLM (formato: VLLM_SERVER_NOMBRE_MODELO=URL)
VLLM_SERVER_OLMOCR=http://vllm-olmocr:8001
VLLM_SERVER_LLAMA=http://vllm-llama:8002
VLLM_SERVER_LLAVA=http://vllm-llava:8003

```

#### ConfiguraciÃ³n del Modelo (`vllm-models/olmocr/.env`)
```bash
# Modelo especÃ­fico
VLLM_MODEL=allenai/olmOCR-7B-0825-FP8
MODEL_NAME=olmocr

# ConfiguraciÃ³n GPU
CUDA_VISIBLE_DEVICES=0
GPU_MEMORY_UTILIZATION=0.9

# Servidor
HOST=0.0.0.0
PORT=8001

# Rendimiento
MAX_MODEL_LEN=8192
TENSOR_PARALLEL_SIZE=1
MAX_NUM_BATCHED_TOKENS=32768
```

## ğŸ”§ Ajustes por GPU

### GPUs con Diferentes Capacidades

```bash
# Para GPUs con 8GB (RTX 3070, RTX 4060 Ti)
GPU_MEMORY_UTILIZATION=0.8
MAX_MODEL_LEN=4096
MAX_NUM_BATCHED_TOKENS=16384

# Para GPUs con 6GB (RTX 3060)
GPU_MEMORY_UTILIZATION=0.7
MAX_MODEL_LEN=2048
MAX_NUM_BATCHED_TOKENS=8192

# Para GPUs con 4GB (GTX 1650)
GPU_MEMORY_UTILIZATION=0.6
MAX_MODEL_LEN=1024
MAX_NUM_BATCHED_TOKENS=4096
```

## ğŸ“¡ Endpoints de la API

### GET /
InformaciÃ³n del servicio y modelos disponibles
```json
{
  "service": "VLLMs Docker API",
  "vllm_servers": {
    "olmocr": "http://vllm-olmocr:8001",
    "llama": "http://vllm-llama:8002"
  },
  "available_models": ["olmocr", "llama"],
  "status": "running"
}
```

### GET /health
Chequeo de salud de todos los servidores vLLM configurados
```json
{
  "status": "healthy",
  "servers_health": {
    "olmocr (http://vllm-olmocr:8001)": true,
    "model2 (http://vllm-model2:8002)": true
  },
  "total_servers": 2,
  "healthy_servers": 2
}
```

### GET /models
Lista de modelos disponibles y su estado
```json
{
  "available_models": {
    "olmocr": {
      "server_url": "http://vllm-olmocr:8001",
      "served_name": "olmocr",
      "healthy": true
    }
  },
  "total_models": 1
}
```

### POST /process
Procesar un archivo PDF o imagen con un modelo especÃ­fico

**ParÃ¡metros:**
- `file` (UploadFile): Archivo PDF o imagen a procesar
- `model` (str): **Requerido** - Nombre del modelo a usar
- `output_format` (str): Formato de salida (`markdown` o `text`)
- `gpu_util` (float): UtilizaciÃ³n de memoria GPU (0.0-1.0)
- `max_len` (int): Longitud mÃ¡xima del modelo

## ğŸ› ï¸ Desarrollo Local

### 1. Instalar Dependencias
```bash
cd vllm-api/app
pip install -r requirements.txt
```

### 2. Iniciar Servidores vLLM

#### Servidor OCR (OlmOCR)
```bash
# Iniciar servidor vLLM con modelo OCR
docker run -d --name vllm-ocr \
  --gpus all \
  -p 8001:8001 \
  vllm/vllm-openai:nightly \
  --model allenai/olmOCR-7B-0825-FP8 \
  --host 0.0.0.0 --port 8001
```

#### Servidor LLM (Llama)
```bash
# Iniciar servidor vLLM con modelo de lenguaje
docker run -d --name vllm-llama \
  --gpus all \
  -p 8002:8002 \
  vllm/vllm-openai:nightly \
  --model meta-llama/Llama-2-7b-chat-hf \
  --host 0.0.0.0 --port 8002
```

#### Servidor VisiÃ³n-Lenguaje (LLaVA)
```bash
# Iniciar servidor vLLM con modelo de visiÃ³n
docker run -d --name vllm-llava \
  --gpus all \
  -p 8003:8003 \
  vllm/vllm-openai:nightly \
  --model llava-hf/llava-1.5-7b-hf \
  --host 0.0.0.0 --port 8003
```

### 3. Iniciar API
```bash
cd vllm-api/app
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

## â• Agregar Nuevo Modelo

### 1. Crear Directorio del Modelo
```bash
cd vllm-models
mkdir nuevo-modelo
```

### 2. Copiar ConfiguraciÃ³n Base
```bash
# Para modelos basados en plantillas existentes
cp -r olmocr/* nuevo-modelo/
# O crear desde cero
```

### 3. Personalizar ConfiguraciÃ³n

#### Para Modelo de Lenguaje (LLM)
```bash
# nuevo-modelo/.env
VLLM_MODEL=meta-llama/Llama-2-7b-chat-hf
MODEL_NAME=llama-7b
MAX_MODEL_LEN=4096
GPU_MEMORY_UTILIZATION=0.8
```

#### Para Modelo de VisiÃ³n (VLM)
```bash
# nuevo-modelo/.env
VLLM_MODEL=llava-hf/llava-1.5-7b-hf
MODEL_NAME=llava
MAX_MODEL_LEN=2048
GPU_MEMORY_UTILIZATION=0.9
```

#### Para Modelo Especializado
```bash
# nuevo-modelo/.env
VLLM_MODEL=codellama/CodeLlama-7b-hf
MODEL_NAME=codellama
MAX_MODEL_LEN=16384
GPU_MEMORY_UTILIZATION=0.85
```

### 4. Probar Nuevo Modelo
```bash
cd vllm-models/nuevo-modelo
docker-compose up -d
docker-compose logs -f
```

## ğŸ” SoluciÃ³n de Problemas

### Problemas Comunes con GPU
```bash
# Verificar instalaciÃ³n GPU
nvidia-smi

# Verificar soporte GPU en Docker
docker run --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi

# Verificar que Docker tenga acceso a GPU
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi

# Limpiar cache de modelos si hay problemas
docker volume rm vllm-models_model_cache

# Verificar logs especÃ­ficos de GPU
docker logs nombre-del-contenedor --tail 50
```

### Problemas de Memoria
```bash
# Reducir parÃ¡metros en archivos .env
GPU_MEMORY_UTILIZATION=0.7
MAX_MODEL_LEN=4096
MAX_NUM_BATCHED_TOKENS=8192

# Reiniciar servicios
docker-compose down
docker-compose up -d
```

### Problemas de Puerto
```bash
# Verificar puertos disponibles (Linux/macOS)
lsof -i :8000
lsof -i :8001

# Cambiar puertos en archivos .env si es necesario
```

## ğŸ“Š Ejemplos de Modelos Disponibles

### Modelos OCR y Documentos
- **OlmOCR:** `allenai/olmOCR-7B-0825-FP8` - OCR avanzado y procesamiento de documentos
- **TrOCR:** `microsoft/trocr-base-printed` - OCR para texto impreso
- **Donut:** `naver-clova-ix/donut-base-finetuned-cord-v2` - Procesamiento de documentos

### Modelos de Lenguaje (LLM)
- **Llama 2:** `meta-llama/Llama-2-7b-chat-hf` - GeneraciÃ³n de texto y chat
- **Mistral:** `mistralai/Mistral-7B-Instruct-v0.1` - Modelo instructivo de alta calidad
- **Falcon:** `tiiuae/falcon-7b-instruct` - Modelo eficiente y rÃ¡pido

### Modelos de VisiÃ³n-Lenguaje
- **LLaVA:** `llava-hf/llava-1.5-7b-hf` - DescripciÃ³n y anÃ¡lisis de imÃ¡genes
- **BLIP:** `Salesforce/blip-image-captioning-base` - GeneraciÃ³n de captions
- **OpenFlamingo:** `openflamingo/OpenFlamingo-3B-vitl-mpt1b` - Modelo visiÃ³n-lenguaje

### Modelos de CÃ³digo
- **CodeLlama:** `codellama/CodeLlama-7b-hf` - GeneraciÃ³n y comprensiÃ³n de cÃ³digo
- **StarCoder:** `bigcode/starcoder` - Modelo especializado en programaciÃ³n
- **DeepSeek Coder:** `deepseek-ai/deepseek-coder-6.7b-base` - Modelo cÃ³digo eficiente

## ğŸ“Š Monitoreo y Logs

### Ver Logs
```bash
# Logs de la API
docker-compose -f vllm-api/docker-compose.yml logs -f

# Logs del modelo vLLM
docker-compose -f vllm-models/olmocr/docker-compose.yml logs -f

# Logs de todos los servicios
docker-compose -f vllm-api/docker-compose.yml logs
docker-compose -f vllm-models/olmocr/docker-compose.yml logs
```

### Estado de Recursos GPU
```bash
# Uso detallado de GPU
nvidia-smi

# Uso de GPU en tiempo real
nvidia-smi -l 1

# Procesos usando GPU
nvidia-smi pmon -i 0

# EstadÃ­sticas de contenedores
docker stats

# InformaciÃ³n del sistema
docker system df

# Memoria GPU por proceso
nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv
```

## ğŸš€ Despliegue en ProducciÃ³n

### Recomendaciones
1. **GPUs Obligatorias**: Esta soluciÃ³n requiere GPUs NVIDIA para funcionar
2. **Configurar lÃ­mites de recursos**: En docker-compose.yml segÃºn capacidad de GPU
3. **Monitoreo**: Implementar logging y mÃ©tricas de GPU
4. **Backups**: Configurar volumen persistente para cache de modelos
5. **Actualizaciones**: Mantener modelos y vLLM actualizados
6. **MÃºltiples GPUs**: Considerar configuraciÃ³n multi-GPU para mayor throughput

### Ejemplo de ProducciÃ³n
```yaml
# docker-compose.production.yml
version: "3.8"
services:
  multi-model-api:
    # ... configuraciÃ³n existente ...
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  vllm-olmocr:
    # ... configuraciÃ³n existente ...
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
```

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n oficial de vLLM](https://docs.vllm.ai/)
- [Modelos disponibles en Hugging Face](https://huggingface.co/models)
- [GuÃ­a de instalaciÃ³n GPU](https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

## ğŸ“„ Licencia

Este proyecto se proporciona como estÃ¡ para fines educativos y de desarrollo.

## ğŸ†˜ Soporte

Para problemas y preguntas:
1. Revisa la secciÃ³n de soluciÃ³n de problemas
2. Consulta la documentaciÃ³n de Docker y vLLM
3. Abre un issue con informaciÃ³n detallada del problema

---

**Â¡Gracias por usar VLLMs Docker API - La plataforma multi-modelo exclusiva para GPU!** ğŸš€