# vLLM Models Directory

Este directorio contiene configuraciones para diferentes modelos que pueden ser ejecutados con vLLM.

## üìÅ Estructura

```
vllm-models/
‚îú‚îÄ‚îÄ README.md                    # Este archivo
‚îú‚îÄ‚îÄ model_cache/                 # Volumen Docker para cache de modelos
‚îî‚îÄ‚îÄ [model-name]/               # Carpeta por modelo
    ‚îú‚îÄ‚îÄ docker-compose.yml      # Configuraci√≥n espec√≠fica del modelo
    ‚îú‚îÄ‚îÄ README.md              # Documentaci√≥n del modelo
    ‚îî‚îÄ‚îÄ .env                   # Variables de entorno del modelo
```

## üöÄ Uso R√°pido

### Para un modelo espec√≠fico:

```bash
cd vllm-models/[model-name]

# Configurar variables de entorno seg√∫n tu GPU
nano .env  # o editar con tu editor preferido

# Iniciar el servidor vLLM
docker-compose up -d

# Ver logs en tiempo real
docker-compose logs -f

# Detener el servidor
docker-compose down
```

## ‚öôÔ∏è Configuraci√≥n por Variables de Entorno

Cada modelo se configura mediante un archivo `.env` que permite ajustar f√°cilmente todos los par√°metros seg√∫n las capacidades de tu GPU:

### Variables Principales

```bash
# Modelo
VLLM_MODEL=allenai/olmOCR-7B-0825-FP8
MODEL_NAME=olmocr

# GPU
CUDA_VISIBLE_DEVICES=0
GPU_MEMORY_UTILIZATION=0.9

# Servidor
HOST=0.0.0.0
PORT=8001

# Rendimiento
MAX_MODEL_LEN=8192
TENSOR_PARALLEL_SIZE=1
MAX_NUM_BATCHED_TOKENS=32768
```

### Para GPUs con Diferentes Capacidades

```bash
# Para GPUs con 8GB (RTX 3070, RTX 4060 Ti)
GPU_MEMORY_UTILIZATION=0.8
MAX_MODEL_LEN=4096
MAX_NUM_BATCHED_TOKENS=16384

# Para GPUs con 6GB (RTX 3060)
GPU_MEMORY_UTILIZATION=0.7
MAX_MODEL_LEN=2048
MAX_NUM_BATCHED_TOKENS=8192

# Para GPUs con 4GB (GTX 1650)
GPU_MEMORY_UTILIZATION=0.6
MAX_MODEL_LEN=1024
MAX_NUM_BATCHED_TOKENS=4096
```

## ‚ûï Agregar Nuevo Modelo

1. Crear directorio:
   ```bash
   mkdir vllm-models/nuevo-modelo
   ```

2. Copiar archivos plantilla:
   ```bash
   cp -r vllm-models/template/* vllm-models/nuevo-modelo/
   ```

3. Personalizar configuraci√≥n:
   - Editar `.env` con el modelo espec√≠fico
   - Actualizar `README.md` con documentaci√≥n
   - Ajustar par√°metros seg√∫n GPU disponible

4. Probar el nuevo modelo:
   ```bash
   cd vllm-models/nuevo-modelo
   docker-compose up -d
   docker-compose logs -f
   ```

## üîß Configuraci√≥n GPU

Todos los modelos est√°n configurados para usar GPU por defecto. Aseg√∫rate de:

- Tener drivers NVIDIA instalados
- Docker con soporte GPU habilitado
- Suficiente memoria GPU disponible

## üìä Modelos Disponibles

### OlmOCR
- **Modelo:** `allenai/olmOCR-7B-0825-FP8`
- **Uso:** OCR y procesamiento de documentos
- **Estado:** ‚úÖ Configurado y listo

## üîç Soluci√≥n de Problemas

### Problemas Comunes

1. **Error de GPU:**
   ```bash
   # Verificar instalaci√≥n GPU
   nvidia-smi
   docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi
   ```

2. **Memoria insuficiente:**
   - Editar `.env` con par√°metros m√°s bajos
   - Reiniciar: `docker-compose down && docker-compose up -d`

3. **Modelo no encontrado:**
   - Verificar conexi√≥n a internet
   - Limpiar cache: `docker volume rm vllm-models_model_cache`
   - Reiniciar: `docker-compose down && docker-compose up -d`

## üìö Recursos

- [Documentaci√≥n vLLM](https://docs.vllm.ai/)
- [Modelos disponibles](https://huggingface.co/models)
- [Gu√≠a instalaci√≥n GPU](https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html)