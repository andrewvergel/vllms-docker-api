version: "3.8"

services:
  vllm-olmocr:
    image: vllm/vllm-openai:nightly
    container_name: ${CONTAINER_NAME}
    command: >
      --model ${VLLM_MODEL_PATH}
      --host ${VLLM_HOST}
      --port ${VLLM_PORT}
      --served-model-name ${VLLM_SERVED_MODEL_NAME}
      --max-model-len ${VLLM_MAX_MODEL_LEN}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}
      --max-num-batched-tokens ${VLLM_MAX_NUM_BATCHED_TOKENS}
      --dtype float16
    ports:
      - "${VLLM_PORT}:${VLLM_PORT}"
    volumes:
      - ../model_cache:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}
      - VLLM_LOGGING_LEVEL=WARNING
      - TORCH_CUDA_ARCH_LIST=7.0;7.5;8.0;8.6;8.9;9.0+PTX
      - NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
    restart: unless-stopped

volumes:
  model_cache:
    external: true
