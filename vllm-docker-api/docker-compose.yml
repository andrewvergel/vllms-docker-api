services:
  vllm-server:
    image: vllm/vllm-openai:nightly
    container_name: vllm-server
    command: --model allenai/olmOCR-7B-0825-FP8 --host 0.0.0.0 --port 8001 --served-model-name olmocr --max-model-len 20480 --gpu-memory-utilization 0.96 --max-num-batched-tokens 49152 --max-num-seqs 192 --tensor-parallel-size 2 --block-size 32 --swap-space 12 --max-parallel-loading-workers 4 --enable-chunked-prefill --disable-log-stats
    ports:
      - "8001:8001"
    volumes:
      - model_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLMOCR_DEVICE=cuda

  olmocr-api:
    build: .
    container_name: olmocr-api
    ports:
      - "8000:8000"
    volumes:
      - ./workspace:/workspace
    depends_on:
      - vllm-server
    environment:
      - OLMOCR_DEVICE=cuda

volumes:
  model_cache:
    driver: local
