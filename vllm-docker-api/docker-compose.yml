services:
  vllm-server:
    image: vllm/vllm-openai:nightly
    container_name: vllm-server
    command: --model allenai/olmOCR-7B-0825-FP8 --host 0.0.0.0 --port 8001 --served-model-name olmocr --max-model-len 24576 --gpu-memory-utilization 0.95 --max-num-batched-tokens 65536 --max-num-seqs 256 --tensor-parallel-size 1 --block-size 16 --swap-space 16 --max-parallel-loading-workers 4 --disable-log-stats --disable-custom-all-reduce
    ports:
      - "8001:8001"
    volumes:
      - ./model_cache:/root/.cache/huggingface
      - /mnt/hf_models:/mnt/hf_models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLMOCR_DEVICE=cuda

  olmocr-api:
    build: .
    container_name: olmocr-api
    ports:
      - "8000:8000"
    volumes:
      - ./workspace:/workspace
    depends_on:
      - vllm-server
    environment:
      - OLMOCR_DEVICE=cuda

volumes:
  model_cache:
    driver: local
